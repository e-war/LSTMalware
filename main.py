import numpy as np
import csv
import re
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import OneHotEncoder
from tensorflow import keras
from tensorflow import Tensor
from keras.models import Sequential
from keras.layers import LSTM, Dense

## variables
dataset_path = '/home/elliot/Development/AI/LSTMalware/datasets/sample/'

include_lines = 5 # how many logs do we take from each file

hidden_units = 10
epochs = 4
batch_size = 100
##


def encode_data(data):
#https://datascience.stackexchange.com/questions/69289/do-i-need-to-convert-strings-before-using-lstm
    # Initialize encoder

    encoder = OneHotEncoder()
    
    # flatten data

    flat_data = np.array(data).reshape(-1,1)

    # encode incoming data

    flat_enc_data = encoder.fit_transform(flat_data)

    #shape data back to original
    print(flat_enc_data)
    encoded_data = flat_enc_data.reshape(data.shape) 

    return encoded_data

    '''
    old code
    # Initialize the LabelEncoder
    label_encoder = LabelEncoder()

    # Flatten and encode the data
    encoded_data = label_encoder.fit_transform(np.array(data).flatten())

    return encoded_data.reshape(data.shape)

    '''   

# read labels.csv from directory, use to construct np array of all collected process within labels.csv
''' dataset format:
time ------------->
[    \/ one operation             \/ one operation
    [[feature1,feature2,feature3],[feature1.feature2,feature3]], <- program 1
    [[feature1,feature2,feature3],[feature1.feature2,feature3]], <- program 2
    [[feature1,feature2,feature3],[feature1.feature2,feature3]],
]
'''
def construct_dataset_fromdir(dir):
    constructed_data = []
    with open(dir+'labels.csv', newline='') as csvfile:
        reader = csv.DictReader(csvfile)
        for process in reader:
            timeline = retrieve_operation_data(dir+process['process_name']+'.CSV',process['process_name']) 
            constructed_data.append(timeline)
    return constructed_data

# convert a single applications csv file into a usable numpy array of the process + created threads, perform this for the first x rows
def retrieve_operation_data(csv_path,process_name):
    process_operations = []
    with open(csv_path, newline='') as csvfile:
        reader = csv.DictReader(csvfile)
        child_threads = []
        for i, row in enumerate(reader):
            # we only do the first x rows
            if(i >= include_lines):
                break
            # if this is monitored process or a child of this process
            if(row['Process Name'] == process_name or row['TID'] in child_threads):
                # if this thread creates a child we should follow this process as well
                if(row['Operation'] == 'Thread Create'):
                    # skips the prefix 'Thread ID: '(len=11) and just appends the thread number <- needs testing
                    child_threads.append(row['Detail'][11:])
                # prepare needed headers to variables
                proc_epath = row['Image Path']
                proc_operation = row['Operation']
                proc_path = row['Path']
                proc_result = row['Result']
                proc_detail = row['Detail']
                proc_duration = row['Duration']
               
                # append operation
                process_operations.append([proc_epath,proc_path,proc_operation,proc_result,proc_detail,proc_duration])
    return process_operations

# takes in standardized feature lists and sends each set of features to the appropriate vectorizer/normalizer
def vectorize_feature_datasets(train_dataset,test_dataset):
    print()

def vectorize_paths():
    print()

def vectorize_operations():
    print()

def vectorize_results():
    print()

def vectorize_details():
    print()

def normalize_durations():
    print()

    
# standardizes + splits text data to be prepared for vectorization < - vectorize features seperately (aside from paths) + only use training data for adapting
def standardize_data(pre_standardized_dataset):
    standardized_array = []
    standardized_array.append(path_lists_to_token_list(pre_standardized_dataset[:,:,0])) # <- standardized epaths [0]
    standardized_array.append(path_lists_to_token_list(pre_standardized_dataset[:,:,1])) # <- standardized paths [1]
    standardized_array.append(multi_to_single_word_list(pre_standardized_dataset[:,:,2])) # <- standarized operations [2]
    standardized_array.append(multi_to_single_word_list(pre_standardized_dataset[:,:,3])) # <- standarized results [3]
    standardized_array.append(details_splitting(pre_standardized_dataset[:,:,4])) # <- standardized details [4]
    standardized_array.append(string_to_float_lists(pre_standardized_dataset[:,:,5])) # <- standardized durations [5]

    return standardized_array


def details_splitting(details_lists):
    standardized_details_list = []
    for x in range(0,len(details_lists)): #<- x = 1 entire timeline
        standardized_details_list.append([]) # create timeline entry
        for y in range(0,len(details_lists[x])):
            tokenized_list = re.split('\, |\: ',details_lists[x][y])
            for k in range(0, len(tokenized_list)):
                detail = tokenized_list[k]
                detail = detail.lower()
                detail = detail.replace(" ", "")
                tokenized_list[k] = detail
            standardized_details_list[x].append(tokenized_list)
    return standardized_details_list
def string_to_float_lists(string_lists):
    standardized_float_list = []
    for x in range(0,len(string_lists)): #<- x = 1 entire timeline
        standardized_float_list.append([]) # create timeline entry
        for y in range(0,len(string_lists[x])): # <- y = 1 instance of activity
            duration = float(string_lists[x][y])
            standardized_float_list[x].append(duration)
    return standardized_float_list
def multi_to_single_word_list(word_list):
    standardized_word_list = []
    for x in range(0,len(word_list)): #<- x = 1 entire timeline
        standardized_word_list.append([]) # create timeline entry
        for y in range(0,len(word_list[x])): # <- y = 1 instance of activity
            word = word_list[x][y].replace(" ","")
            word = word.lower()
            standardized_word_list[x].append(word)
    return standardized_word_list
def path_lists_to_token_list(path_list): # in: a list of all paths, out: a list of lists containing the paths seperated by the '\'
    standardized_path_list = []
    for x in range(0,len(path_list)): #<- x = 1 entire timeline
        standardized_path_list.append([]) # create timeline entry
        for y in range(0,len(path_list[x])): # <- y = 1 instance of activity
            standardized_path_list[x].append(split_path_data_to_tokens(path_list[x][y]))
    return standardized_path_list
def split_path_data_to_tokens(path_data):
    # drop first 3 chars (for windows drive)  + split paths as a list of strings by '/' (token)
    if(path_data[:3] == 'C:\\'):
        path_data = path_data[3:]
    path_data = re.split('\\\\|\.',path_data)
    for x in range(0, len(path_data)):
        path_data[x] = path_data[x].lower()
    return path_data

## testing
base_dataset = construct_dataset_fromdir(dataset_path)
base_dataset = np.array(base_dataset)

labels = [
    "BENIGN", # prog 1
    "VIRUS" # prog 2
]

label_e = LabelEncoder()
enc_labels = label_e.fit_transform(labels)

x_train, x_test, y_train, y_test = train_test_split(base_dataset ,enc_labels, test_size=0.5, random_state=55) # <- currently set as .5 as we have 2 programs as test data, one for each dataset.

standardized_train = standardize_data(x_train)
standardized_test = standardize_data(x_test)



print(standardized_train)

#enc_data = encode_data(data)
#print(enc_data)

'''
outline:
data = dataset_fromdir
x_train, x_test = split(data)

// these following standardized lists are [[ALL FEATURE1 TENSOR],[ALL FEATURE2 TENSOR],..] 
// these must be reshaped into the original datasets shape once vectorization/normalization has taken place.

standard_train = standardize(x_train)  
standard_test = standardize(x_test)

// the vectorization process should take both train and test data simultaniously but only adapt based
// on the training data, this will prevent the model from having any unseen knowledge.

vectorized_train, vectorized_test = vectorize(standard_train,standard_test)

// once vectorization is finished then both datasets should be converted back to the original tensor shape

rebuilt_train, rebuilt_test = rebuild_dataset(vectorized_train, vectorized_test)

// after this there should be 2 datasets 

rebuilt_train=[
    [[feature1_vectorized,feature2_vectorized],[feature1_vectorized,feature2_vectorized]],
    [[feature1_vectorized,feature2_vectorized],[feature1_vectorized,feature2_vectorized]]
]

rebuilt_test=[
    [[feature1_vectorized,feature2_vectorized],[feature1_vectorized,feature2_vectorized]],
    [[feature1_vectorized,feature2_vectorized],[feature1_vectorized,feature2_vectorized]]
]



'''

exit()
enc_data = encode_data(data)
print(enc_data)




## reshape data for LSTM input

x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], -1))
x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], -1))

y_train = np.asarray(y_train).astype('float32').reshape((-1,1))
y_test = np.asarray(y_test).astype('float32').reshape((-1,1))



model = Sequential()
model.add(LSTM(hidden_units,input_shape=(x_train.shape[1], x_train.shape[2])))
model.add(Dense(1,activation='sigmoid'))
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
model.summary()
print(model.dtype)
model.fit(x_train, y_train, validation_data=(x_test, y_test),epochs=epochs, batch_size=batch_size)
