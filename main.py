import numpy as np
import csv
import re
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import OneHotEncoder
from tensorflow import keras
from tensorflow import Tensor
from keras.models import Sequential
from keras.layers import LSTM, Dense

## variables
dataset_path = '/home/elliot/Development/AI/LSTMalware/datasets/sample/'

include_lines = 5 # how many logs do we take from each file

hidden_units = 10
epochs = 4
batch_size = 100
##


def encode_data(data):
#https://datascience.stackexchange.com/questions/69289/do-i-need-to-convert-strings-before-using-lstm
    # Initialize encoder

    encoder = OneHotEncoder()
    
    # flatten data

    flat_data = np.array(data).reshape(-1,1)

    # encode incoming data

    flat_enc_data = encoder.fit_transform(flat_data)

    #shape data back to original
    print(flat_enc_data)
    encoded_data = flat_enc_data.reshape(data.shape) 

    return encoded_data

    '''
    old code
    # Initialize the LabelEncoder
    label_encoder = LabelEncoder()

    # Flatten and encode the data
    encoded_data = label_encoder.fit_transform(np.array(data).flatten())

    return encoded_data.reshape(data.shape)

    '''   

# read labels.csv from directory, use to construct np array of all collected process within labels.csv

def construct_dataset_fromdir(dir):
    constructed_data = []
    with open(dir+'labels.csv', newline='') as csvfile:
        reader = csv.DictReader(csvfile)
        for process in reader:
            timeline = retrieve_operation_data(dir+process['process_name']+'.CSV',process['process_name']) 
            constructed_data.append(timeline)
    return constructed_data

# convert a single applications csv file into a usable numpy array of the process + created threads, perform this for the first x rows
def retrieve_operation_data(csv_path,process_name):
    process_operations = []
    with open(csv_path, newline='') as csvfile:
        reader = csv.DictReader(csvfile)
        child_threads = []
        for i, row in enumerate(reader):
            # we only do the first x rows
            if(i >= include_lines):
                break
            # if this is monitored process or a child of this process
            if(row['Process Name'] == process_name or row['TID'] in child_threads):
                # if this thread creates a child we should follow this process as well
                if(row['Operation'] == 'Thread Create'):
                    # skips the prefix 'Thread ID: '(len=11) and just appends the thread number <- needs testing
                    child_threads.append(row['Detail'][11:])
                # prepare needed headers to variables
                proc_epath = row['Image Path']
                proc_operation = row['Operation']
                proc_path = row['Path']
                proc_result = row['Result']
                proc_detail = row['Detail']
                proc_duration = row['Duration']
               
                # append operation
                process_operations.append([proc_epath,proc_path,proc_operation,proc_result,proc_detail,proc_duration])
    return process_operations

# standardizes + splits text data to be prepared for vectorization < - vectorize features seperately (aside from paths) + only use training data for adapting
def standardize_data(pre_standardized_dataset):
    standardized_dataset = pre_standardized_dataset
    # Standardize Paths
    for pid, program in enumerate(pre_standardized_dataset[:,:,0]): # all "epath features", program is one timeline
        for iid, ind in enumerate(program): # ind = one epath
            tokenized_data = np.array(split_path_data_to_tokens(pre_standardized_dataset[:,:,0][pid][iid]))

    pre_standardized_dataset[:,:,1] # all "path features"
    pre_standardized_dataset[:,:,2] # all "operation features"
    pre_standardized_dataset[:,:,3] # all "result features"
    pre_standardized_dataset[:,:,4] # all "detail features"
    pre_standardized_dataset[:,:,5] # all "duration features"

    #print(standardized_dataset[:,:,0])
        # lowercase + combine operation strings (single string token)
    # lowercase + combine result strings (single string token)
    # if float of duration = 0 replace the last char of the string to 1 and then place as float <- we need to keep this data seperate prior to vectorization as this should be a float for the network to read rather than a string. 
    # details? 

def split_path_data_to_tokens(path_data):
    # drop first 3 chars (for windows drive)  + split paths as a list of strings by '/' (token)
    if(path_data[:3] == 'C:\\'):
        path_data = path_data[3:]
    path_data = re.split('\\\\|\.',path_data)
    return path_data

def testing(array):
    for pid, program in enumerate(array[:,:,5]):
        for iid, ind in enumerate(program):
            print(array[:,:,1][pid][iid])
## testing
data = construct_dataset_fromdir(dataset_path)
data = np.array(data)
standardize_data(data)
#enc_data = encode_data(data)
#print(enc_data)

exit()
enc_data = encode_data(data)
print(enc_data)


labels = [
    "SAFE", # prog 1
    "MALICIOUS" # prog 2
]

label_e = LabelEncoder()
enc_labels = label_e.fit_transform(labels)

x_train, x_test, y_train, y_test = train_test_split(enc_data ,enc_labels, test_size=0.2, random_state=55)

## reshape data for LSTM input

x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], -1))
x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], -1))

y_train = np.asarray(y_train).astype('float32').reshape((-1,1))
y_test = np.asarray(y_test).astype('float32').reshape((-1,1))



model = Sequential()
model.add(LSTM(hidden_units,input_shape=(x_train.shape[1], x_train.shape[2])))
model.add(Dense(1,activation='sigmoid'))
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
model.summary()
print(model.dtype)
model.fit(x_train, y_train, validation_data=(x_test, y_test),epochs=epochs, batch_size=batch_size)
