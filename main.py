import numpy as np
import csv
import re
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import OneHotEncoder
from tensorflow import keras
from tensorflow import Tensor
from keras.models import Sequential
from keras.layers import LSTM, Dense

## variables
dataset_path = '/home/elliot/Development/AI/LSTMalware/datasets/sample/'

include_lines = 5 # how many logs do we take from each file

hidden_units = 10
epochs = 4
batch_size = 100
##


def encode_data(data):
#https://datascience.stackexchange.com/questions/69289/do-i-need-to-convert-strings-before-using-lstm
    # Initialize encoder

    encoder = OneHotEncoder()
    
    # flatten data

    flat_data = np.array(data).reshape(-1,1)

    # encode incoming data

    flat_enc_data = encoder.fit_transform(flat_data)

    #shape data back to original
    print(flat_enc_data)
    encoded_data = flat_enc_data.reshape(data.shape) 

    return encoded_data

    '''
    old code
    # Initialize the LabelEncoder
    label_encoder = LabelEncoder()

    # Flatten and encode the data
    encoded_data = label_encoder.fit_transform(np.array(data).flatten())

    return encoded_data.reshape(data.shape)

    '''   

# read labels.csv from directory, use to construct np array of all collected process within labels.csv
''' dataset format:
time ------------->
[    \/ one operation             \/ one operation
    [[feature1,feature2,feature3],[feature1.feature2,feature3]], <- program 1
    [[feature1,feature2,feature3],[feature1.feature2,feature3]], <- program 2
    [[feature1,feature2,feature3],[feature1.feature2,feature3]],
]
'''
def construct_dataset_fromdir(dir):
    constructed_data = []
    with open(dir+'labels.csv', newline='') as csvfile:
        reader = csv.DictReader(csvfile)
        for process in reader:
            timeline = retrieve_operation_data(dir+process['process_name']+'.CSV',process['process_name']) 
            constructed_data.append(timeline)
    return constructed_data

# convert a single applications csv file into a usable numpy array of the process + created threads, perform this for the first x rows
def retrieve_operation_data(csv_path,process_name):
    process_operations = []
    with open(csv_path, newline='') as csvfile:
        reader = csv.DictReader(csvfile)
        child_threads = []
        for i, row in enumerate(reader):
            # we only do the first x rows
            if(i >= include_lines):
                break
            # if this is monitored process or a child of this process
            if(row['Process Name'] == process_name or row['TID'] in child_threads):
                # if this thread creates a child we should follow this process as well
                if(row['Operation'] == 'Thread Create'):
                    # skips the prefix 'Thread ID: '(len=11) and just appends the thread number <- needs testing
                    child_threads.append(row['Detail'][11:])
                # prepare needed headers to variables
                proc_epath = row['Image Path']
                proc_operation = row['Operation']
                proc_path = row['Path']
                proc_result = row['Result']
                proc_detail = row['Detail']
                proc_duration = row['Duration']
               
                # append operation
                process_operations.append([proc_epath,proc_path,proc_operation,proc_result,proc_detail,proc_duration])
    return process_operations

# standardizes + splits text data to be prepared for vectorization < - vectorize features seperately (aside from paths) + only use training data for adapting
def standardize_data(pre_standardized_dataset):
    
    standardized_epaths = path_lists_to_token_list(pre_standardized_dataset[:,:,0])# <- split token data ready for vectorization
    standardized_paths = path_lists_to_token_list(pre_standardized_dataset[:,:,1])# <- split token data ready for vectorization
    standardized_operations = multi_to_single_word_list(pre_standardized_dataset[:,:,2])
    standardized_results = multi_to_single_word_list(pre_standardized_dataset[:,:,3])
    standardized_details = details_splitting(pre_standardized_dataset[:,:,4])
    standardized_durations = string_to_float_lists(pre_standardized_dataset[:,:,5])
    print(standardized_details)


def details_splitting(details_lists):
    standardized_details_list = []
    for x in range(0,len(details_lists)): #<- x = 1 entire timeline
        standardized_details_list.append([]) # create timeline entry
        for y in range(0,len(details_lists[x])):
            tokenized_list = re.split('\, |\: ',details_lists[x][y])
            for k in range(0, len(tokenized_list)):
                detail = tokenized_list[k]
                detail = detail.lower()
                detail = detail.replace(" ", "")
                tokenized_list[k] = detail
            standardized_details_list[x].append(tokenized_list)
    return standardized_details_list
def string_to_float_lists(string_lists):
    standardized_float_list = []
    for x in range(0,len(string_lists)): #<- x = 1 entire timeline
        standardized_float_list.append([]) # create timeline entry
        for y in range(0,len(string_lists[x])): # <- y = 1 instance of activity
            duration = float(string_lists[x][y])
            standardized_float_list[x].append(duration)
    return standardized_float_list
def multi_to_single_word_list(word_list):
    standardized_word_list = []
    for x in range(0,len(word_list)): #<- x = 1 entire timeline
        standardized_word_list.append([]) # create timeline entry
        for y in range(0,len(word_list[x])): # <- y = 1 instance of activity
            word = word_list[x][y].replace(" ","")
            word = word.lower()
            standardized_word_list[x].append(word)
    return standardized_word_list
def path_lists_to_token_list(path_list): # in: a list of all paths, out: a list of lists containing the paths seperated by the '\'
    standardized_path_list = []
    for x in range(0,len(path_list)): #<- x = 1 entire timeline
        standardized_path_list.append([]) # create timeline entry
        for y in range(0,len(path_list[x])): # <- y = 1 instance of activity
            standardized_path_list[x].append(split_path_data_to_tokens(path_list[x][y]))
    return standardized_path_list
def split_path_data_to_tokens(path_data):
    # drop first 3 chars (for windows drive)  + split paths as a list of strings by '/' (token)
    if(path_data[:3] == 'C:\\'):
        path_data = path_data[3:]
    path_data = re.split('\\\\|\.',path_data)
    for x in range(0, len(path_data)):
        path_data[x] = path_data[x].lower()
    return path_data

## testing
data = construct_dataset_fromdir(dataset_path)
data = np.array(data)
standardize_data(data)
#enc_data = encode_data(data)
#print(enc_data)

exit()
enc_data = encode_data(data)
print(enc_data)


labels = [
    "SAFE", # prog 1
    "MALICIOUS" # prog 2
]

label_e = LabelEncoder()
enc_labels = label_e.fit_transform(labels)

x_train, x_test, y_train, y_test = train_test_split(enc_data ,enc_labels, test_size=0.2, random_state=55)

## reshape data for LSTM input

x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], -1))
x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], -1))

y_train = np.asarray(y_train).astype('float32').reshape((-1,1))
y_test = np.asarray(y_test).astype('float32').reshape((-1,1))



model = Sequential()
model.add(LSTM(hidden_units,input_shape=(x_train.shape[1], x_train.shape[2])))
model.add(Dense(1,activation='sigmoid'))
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
model.summary()
print(model.dtype)
model.fit(x_train, y_train, validation_data=(x_test, y_test),epochs=epochs, batch_size=batch_size)
