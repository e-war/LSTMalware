import numpy as np
import csv
import re
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import OneHotEncoder
from tensorflow import keras
from tensorflow import Tensor
from tensorflow import data
from keras.models import Sequential
from keras.layers import LSTM, Dense, TextVectorization

## variables
dataset_path = '/home/elliot/Development/AI/LSTMalware/datasets/sample/'

include_lines = 5 # how many logs do we take from each file

hidden_units = 10
epochs = 4
batch_size = 100
##


def encode_data(data):
#https://datascience.stackexchange.com/questions/69289/do-i-need-to-convert-strings-before-using-lstm
    # Initialize encoder

    encoder = OneHotEncoder()
    
    # flatten data

    flat_data = np.array(data).reshape(-1,1)

    # encode incoming data

    flat_enc_data = encoder.fit_transform(flat_data)

    #shape data back to original
    print(flat_enc_data)
    encoded_data = flat_enc_data.reshape(data.shape) 

    return encoded_data

    '''
    old code
    # Initialize the LabelEncoder
    label_encoder = LabelEncoder()

    # Flatten and encode the data
    encoded_data = label_encoder.fit_transform(np.array(data).flatten())

    return encoded_data.reshape(data.shape)

    '''   

# read labels.csv from directory, use to construct np array of all collected process within labels.csv
''' dataset format:
time ------------->
[    \/ one operation             \/ one operation
    [[feature1,feature2,feature3],[feature1.feature2,feature3]], <- program 1
    [[feature1,feature2,feature3],[feature1.feature2,feature3]], <- program 2
    [[feature1,feature2,feature3],[feature1.feature2,feature3]],
]
'''
def construct_dataset_fromdir(dir):
    constructed_data = []
    with open(dir+'labels.csv', newline='') as csvfile:
        reader = csv.DictReader(csvfile)
        for process in reader:
            timeline = retrieve_operation_data(dir+process['process_name']+'.CSV',process['process_name']) 
            constructed_data.append(timeline)
    return constructed_data

# convert a single applications csv file into a usable numpy array of the process + created threads, perform this for the first x rows
def retrieve_operation_data(csv_path,process_name):
    process_operations = []
    with open(csv_path, newline='') as csvfile:
        reader = csv.DictReader(csvfile)
        child_threads = []
        for i, row in enumerate(reader):
            # we only do the first x rows
            if(i >= include_lines):
                break
            # if this is monitored process or a child of this process
            if(row['Process Name'] == process_name or row['TID'] in child_threads):
                # if this thread creates a child we should follow this process as well
                if(row['Operation'] == 'Thread Create'):
                    # skips the prefix 'Thread ID: '(len=11) and just appends the thread number <- needs testing
                    child_threads.append(row['Detail'][11:])
                # prepare needed headers to variables
                proc_epath = row['Image Path']
                proc_operation = row['Operation']
                proc_path = row['Path']
                proc_result = row['Result']
                proc_detail = row['Detail']
                proc_duration = row['Duration']
               
                # append operation
                process_operations.append([proc_epath,proc_path,proc_operation,proc_result,proc_detail,proc_duration])
    return process_operations

# takes in standardized feature lists and sends each set of features to the appropriate vectorizer/normalizer
def vectorize_feature_datasets(train_dataset,test_dataset):
    print("Running Vectorization...")
    print("Vectorizing epaths...")
    vectorize_paths(train_dataset[0],test_dataset[0])

def vectorize_paths(train_path, test_path):
    #testing
    train_path = np.array(train_path)
    flattened_training_data = train_path.reshape(-1)
    vectorizer = TextVectorization(standardize=None,split=None,output_mode="tf_idf")
    vectorizer.adapt(flattened_training_data)
    
    print(vectorizer.get_vocabulary())
    exit()
    '''
    outline:
    training_wordlist = flatten_to_ragged(train_path) <- this is just for our adapt method 

    '''
    
    vectorizer = TextVectorization(standardize=None,split=None,output_mode="tf_idf")
    vectorizer.adapt(train_path)
    print()

def vectorize_operations():
    print()

def vectorize_results():
    print()

def vectorize_details():
    print()

def normalize_durations():
    print()


# standardizes + splits text data to be prepared for vectorization < - vectorize features seperately (aside from paths) + only use training data for adapting
def standardize_data(pre_standardized_dataset):
    standardized_array = []
    standardized_array.append(path_lists_to_token_list(pre_standardized_dataset[:,:,0])) # <- standardized epaths [0]
    standardized_array.append(path_lists_to_token_list(pre_standardized_dataset[:,:,1])) # <- standardized paths [1]
    standardized_array.append(multi_to_single_word_list(pre_standardized_dataset[:,:,2])) # <- standarized operations [2]
    standardized_array.append(multi_to_single_word_list(pre_standardized_dataset[:,:,3])) # <- standarized results [3]
    standardized_array.append(details_splitting(pre_standardized_dataset[:,:,4])) # <- standardized details [4]
    standardized_array.append(string_to_float_lists(pre_standardized_dataset[:,:,5])) # <- standardized durations [5]

    return standardized_array


def details_splitting(details_lists):
    standardized_details_list = []
    for x in range(0,len(details_lists)): #<- x = 1 entire timeline
        standardized_details_list.append([]) # create timeline entry
        for y in range(0,len(details_lists[x])):
            tokenized_list = re.split('\, |\: ',details_lists[x][y])
            for k in range(0, len(tokenized_list)):
                detail = tokenized_list[k]
                detail = detail.lower()
                detail = detail.replace(" ", "")
                tokenized_list[k] = detail
            standardized_details_list[x].append(tokenized_list)
    return standardized_details_list
def string_to_float_lists(string_lists):
    standardized_float_list = []
    for x in range(0,len(string_lists)): #<- x = 1 entire timeline
        standardized_float_list.append([]) # create timeline entry
        for y in range(0,len(string_lists[x])): # <- y = 1 instance of activity
            duration = float(string_lists[x][y])
            standardized_float_list[x].append(duration)
    return standardized_float_list
def multi_to_single_word_list(word_list):
    standardized_word_list = []
    for x in range(0,len(word_list)): #<- x = 1 entire timeline
        standardized_word_list.append([]) # create timeline entry
        for y in range(0,len(word_list[x])): # <- y = 1 instance of activity
            word = word_list[x][y].replace(" ","")
            word = word.lower()
            standardized_word_list[x].append(word)
    return standardized_word_list
def path_lists_to_token_list(path_list): # in: a list of all paths, out: a list of lists containing the paths seperated by the '\'
    standardized_path_list = []
    print(next(iter(path_list)).numpy())
    print("IN")
    exit()
    for x in list(path_list):
        tokenized_list = split_path_data_to_tokens(x.numpy().decode('utf-8'))
        standardized_path_list.append(tokenized_list)
    standardized_path_list = np.array(standardized_path_list)
    print(standardized_path_list)
    return standardized_path_list
    
    for x in range(0,len(path_list)): #<- x = 1 entire timeline
        standardized_path_list.append([]) # create timeline entry
        for y in range(0,len(path_list[x])): # <- y = 1 instance of activity
            standardized_path_list[x].append(split_path_data_to_tokens(path_list[x][y]))
    return standardized_path_list
def split_path_data_to_tokens(path_data):
    # drop first 3 chars (for windows drive)  + split paths as a list of strings by '/' (token)
    if(path_data[:3] == 'C:\\'):
        path_data = path_data[3:]
    path_data = re.split('\\\\|\.',path_data)
    for x in range(0, len(path_data)):
        path_data[x] = path_data[x].lower()
    return path_data

def testing(input):
    test_array = []
    for i, x in enumerate(input):
        test_array.append([])
        for y in x:
            mapped_strings = split_path_data_to_tokens(y[0].numpy().decode('utf-8'))
            mapped_strings = np.asarray(mapped_strings,object).astype('U')
            test_array[i].append([mapped_strings,y[1].numpy().decode('utf-8'),y[2].numpy().decode('utf-8'),y[3].numpy().decode('utf-8'),y[4].numpy().decode('utf-8'),y[5].numpy().decode('utf-8')])
    test_out = np.array(test_array,object)

    print(test_out)
    return test_out
## testing
base_dataset = construct_dataset_fromdir(dataset_path)
base_dataset = np.array(base_dataset)

t1 = tf.map_fn(fn=lambda x: x ,elems=base_dataset[:,:,0])
t2 = tf.map_fn(fn=lambda x: x ,elems=base_dataset[:,:,1])


test_dataset = data.Dataset.from_tensor_slices(base_dataset)
#t1 = test_dataset.map(tf.py_function(testing,[test_dataset],object))
t1 = tf.py_function(testing,[test_dataset],object)
list(t1.as_numpy_iterator())
#mod_dataset = test_dataset.map(lambda x: tf.concat([x[:,:,:]])


#tf.map_fn(fn=lambda t: path_lists_to_token_list(t),elems=base_dataset[:,:,0],fn_output_signature=tf.RaggedTensorSpec(shape=(5,),dtype=tf.string))
#print(t)
exit()

# --- need to do standardization on each feature while keeping the dataset the same shape (REWRITE CURRENT standardization FUNCTIONS TO ACCOMMODATE) use dataset.map


epath_features = test_dataset.map(lambda x: tf.concat([x[:,:,:1],tf.expand_dims(path_lists_to_token_list(x[:,:,0]),axis=-1),x[:,:,1:]],axis=-1))
exit()
for x in epath_features.as_numpy_iterator():
    print(x)

exit()

# -- then need to split datasets here post standardization to produce training/test datasets for adapting vectorization layers + for model training in general.


# -- take the map of each feature, apply it to the appropriate vector/normalizing function and map the results back  
epath_features = test_dataset.map(lambda x: x[:,:,0])

test = path_lists_to_token_list(epath_features)

for x in epath_features:
    print(x)
vectorizer = TextVectorization(standardize=None,split=None,output_mode="tf_idf")
vectorizer.adapt(epath_features)

## -- remap the vectored text back to original shape


print(vectorizer.get_vocabulary())
exit()








labels = [
    "BENIGN", # prog 1
    "VIRUS" # prog 2
]

label_e = LabelEncoder()
enc_labels = label_e.fit_transform(labels)

x_train, x_test, y_train, y_test = train_test_split(base_dataset ,enc_labels, test_size=0.5, random_state=55) # <- currently set as .5 as we have 2 programs as test data, one for each dataset.
x_train = np.array(x_train)
print(x_train.shape)

standardized_train = standardize_data(x_train)
standardized_test = standardize_data(x_test)

#vectorize_feature_datasets(standardized_train,standardized_test)


#enc_data = encode_data(data)
#print(enc_data)

'''
outline:
data = dataset_fromdir
x_train, x_test = split(data)

// these following standardized lists are [[ALL FEATURE1 TENSOR],[ALL FEATURE2 TENSOR],..] 
// these must be reshaped into the original datasets shape once vectorization/normalization has taken place.

standard_train = standardize(x_train)  
standard_test = standardize(x_test)

// the vectorization process should take both train and test data simultaniously but only adapt based
// on the training data, this will prevent the model from having any unseen knowledge.

vectorized_train, vectorized_test = vectorize(standard_train,standard_test)

// once vectorization is finished then both datasets should be converted back to the original tensor shape

rebuilt_train, rebuilt_test = rebuild_dataset(vectorized_train, vectorized_test)

// after this there should be 2 datasets 

rebuilt_train=[
    [[feature1_vectorized,feature2_vectorized],[feature1_vectorized,feature2_vectorized]],
    [[feature1_vectorized,feature2_vectorized],[feature1_vectorized,feature2_vectorized]]
]

rebuilt_test=[
    [[feature1_vectorized,feature2_vectorized],[feature1_vectorized,feature2_vectorized]],
    [[feature1_vectorized,feature2_vectorized],[feature1_vectorized,feature2_vectorized]]
]



'''

exit()
enc_data = encode_data(data)
print(enc_data)




## reshape data for LSTM input

x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], -1))
x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], -1))

y_train = np.asarray(y_train).astype('float32').reshape((-1,1))
y_test = np.asarray(y_test).astype('float32').reshape((-1,1))



model = Sequential()

model.add(LSTM(hidden_units,input_shape=(include_lines, 6)))
model.add(Dense(1,activation='sigmoid'))
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
model.summary()
print(model.dtype)
model.fit(x_train, y_train, validation_data=(x_test, y_test),epochs=epochs, batch_size=batch_size)
