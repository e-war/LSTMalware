import numpy as np
import csv
import re
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import OneHotEncoder
from tensorflow import keras
from tensorflow import Tensor
from tensorflow import data
from keras.models import Sequential
from keras.layers import LSTM, Dense, TextVectorization

## variables
dataset_path = '/home/elliot/Development/AI/LSTMalware/datasets/sample/'

include_lines = 5 # how many logs do we take from each file
test_split = 0.5 # how do we split test / train data.


hidden_units = 10
epochs = 4
batch_size = 100
##


# read labels.csv from directory, use to construct np array of all collected process within labels.csv
''' in_dataset format:
time ------------->
[    \/ one operation             \/ one operation
    [[feature1,feature2,feature3],[feature1.feature2,feature3]], <- program 1
    [[feature1,feature2,feature3],[feature1.feature2,feature3]], <- program 2
    [[feature1,feature2,feature3],[feature1.feature2,feature3]],
]

the model will read from multiple inputs, each input comes from one of the features 
//should the input be just for one instance in a sequence or the whole sequence as it will just be a time sequence of one feature?
//does each input need to be labelled the same? or can i label all of the inputs as one?

[vectorized_path] -> input(path)
[vectorized_details] -> input(details)

'''
def construct_dataset_fromdir(dir):
    constructed_data = []
    with open(dir+'labels.csv', newline='') as csvfile:
        reader = csv.DictReader(csvfile)
        for process in reader:
            timeline = retrieve_operation_data(dir+process['process_name']+'.CSV',process['process_name']) 
            constructed_data.append(timeline)
    return constructed_data

# convert a single applications csv file into a usable numpy array of the process + created threads, perform this for the first x rows
def retrieve_operation_data(csv_path,process_name):
    process_operations = []
    with open(csv_path, newline='') as csvfile:
        reader = csv.DictReader(csvfile)
        child_threads = []
        for i, row in enumerate(reader):
            # we only do the first x rows
            if(i >= include_lines):
                break
            # if this is monitored process or a child of this process
            if(row['Process Name'] == process_name or row['TID'] in child_threads):
                # if this thread creates a child we should follow this process as well
                if(row['Operation'] == 'Thread Create'):
                    # skips the prefix 'Thread ID: '(len=11) and just appends the thread number <- needs testing
                    child_threads.append(row['Detail'][11:])
                # prepare needed headers to variables
                proc_epath = row['Image Path']
                proc_operation = row['Operation']
                proc_path = row['Path']
                proc_result = row['Result']
                proc_detail = row['Detail']
                proc_duration = row['Duration']
               
                # append operation
                process_operations.append([proc_epath,proc_path,proc_operation,proc_result,proc_detail,proc_duration])
    return process_operations

# takes in standardized feature lists and sends each set of features to the appropriate vectorizer/normalizer
def vectorize_feature_datasets(normalized_datalists):
    print("Running Vectorization...")
    print("Vectorizing epaths...")
    v_epath_train, v_epath_test = vectorize_epaths(normalized_datalists[0])
    print("Vectorizing paths...")
    v_path_train, v_path_test = vectorize_epaths(normalized_datalists[1])
    print("Vectorizing operations...")
    v_op_train, v_op_test = vectorize_operations(normalized_datalists[2])
    print("Vectorizing results...")
    print(normalized_datalists[3])
    v_res_train, v_res_test = vectorize_results(normalized_datalists[3])
    print("Vectorizing details...")
    print("Normalising durations...")
    
    #t1 = tf.keras.layers.Concatenate()([v_epath_train,v_path_train,v_op_train,v_res_train])
    exit()

def vectorize_epaths(epath_dataSet):
    epath_train, epath_test = train_test_split(epath_dataSet, test_size=test_split, random_state=55) # split our input data, we must do this for each input and ensure that they are all the same size
    # create our tensors of data, using ragged as technically they are the same shape but within the lists could be a different amount of text
    epath_vectorTrain = tf.ragged.constant(epath_train)
    epath_train = tf.ragged.constant(epath_train)
    epath_test = tf.ragged.constant(epath_test)
    #just the values of training data used for adapting
    epath_vectorTrain = epath_vectorTrain.flat_values


    epath_vectorizer = keras.layers.TextVectorization(standardize=None,split=None,output_mode="tf_idf")
    epath_vectorizer.adapt(epath_vectorTrain)
    epath_vocab = epath_vectorizer.get_vocabulary()
    print("Vocab includes: ",epath_vocab)
    vect_epath = epath_vectorizer(epath_train.values)
    vect_epath_train = tf.reshape(vect_epath, (epath_train.shape[0],include_lines,len(epath_vocab))) # reshapes the vectorized output into a 3d tensor, where each top level is next process
    vect_epath = epath_vectorizer(epath_test.values)
    vect_epath_test = tf.reshape(vect_epath, (epath_test.shape[0],include_lines,len(epath_vocab))) # reshapes the vectorized output into a 3d tensor, where each top level is next process

    return(vect_epath_train,vect_epath_test) # <- vectorized test / train data

def vectorize_operations(operation_dataSet):
    op_train, op_test = train_test_split(operation_dataSet, test_size=test_split, random_state=55) # split our input data, we must do this for each input and ensure that they are all the same size
    
    op_trainVocab = tf.ragged.constant(op_train).flat_values
    op_train = tf.convert_to_tensor(op_train)
    op_test = tf.convert_to_tensor(op_test)

    

    operation_vectorizer = keras.layers.TextVectorization(standardize=None,split=None,output_mode="int")
    operation_vectorizer.adapt(op_trainVocab)
    operation_vocab = operation_vectorizer.get_vocabulary()
    print("Vocab includes: ",operation_vocab)


    v_op_train = operation_vectorizer(op_train)
    v_op_train = tf.reshape(v_op_train,(len(v_op_train),include_lines,1))
    v_op_test = operation_vectorizer(op_test)
    v_op_test = tf.reshape(v_op_test,(len(v_op_test),include_lines,1))
    return(v_op_train,v_op_test)
    

def vectorize_results(result_dataSet):
    res_train, res_test = train_test_split(result_dataSet, test_size=test_split, random_state=55)

    res_trainVocab = tf.ragged.constant(res_train).flat_values
    res_train = tf.convert_to_tensor(res_train)
    res_test = tf.convert_to_tensor(res_test)

    result_vectorizer = keras.layers.TextVectorization(standardize=None,split=None,output_mode="int")
    result_vectorizer.adapt(res_trainVocab)
    result_vocab = result_vectorizer.get_vocabulary()
    print("Vocab includes: ",result_vocab)

    v_res_train = result_vectorizer(res_train)
    v_res_train = tf.reshape(v_res_train,(len(v_res_train),include_lines,1))
    v_res_test = result_vectorizer(res_test)
    v_res_test = tf.reshape(v_res_test,(len(v_res_test),include_lines,1))

    return(v_res_train,v_res_test)

def vectorize_details(detail_dataSet):
    print()

def normalize_durations():
    print()


# standardizes + splits text data to be prepared for vectorization < - vectorize features seperately (aside from paths) + only use training data for adapting
def standardize_data(pre_standardized_dataset):
    standardized_array = []
    standardized_array.append(path_lists_to_token_list(pre_standardized_dataset[:,:,0])) # <- standardized epaths [0]
    standardized_array.append(path_lists_to_token_list(pre_standardized_dataset[:,:,1])) # <- standardized paths [1]
    standardized_array.append(multi_to_single_word_list(pre_standardized_dataset[:,:,2])) # <- standarized operations [2]
    standardized_array.append(multi_to_single_word_list(pre_standardized_dataset[:,:,3])) # <- standarized results [3]
    standardized_array.append(details_splitting(pre_standardized_dataset[:,:,4])) # <- standardized details [4]
    standardized_array.append(string_to_float_lists(pre_standardized_dataset[:,:,5])) # <- standardized durations [5]
    return standardized_array


def details_splitting(details_lists):
    standardized_details_list = []
    for x in range(0,len(details_lists)): #<- x = 1 entire timeline
        standardized_details_list.append([]) # create timeline entry
        for y in range(0,len(details_lists[x])):
            tokenized_list = re.split('\, |\: ',details_lists[x][y])
            for k in range(0, len(tokenized_list)):
                detail = tokenized_list[k]
                detail = detail.lower()
                detail = detail.replace(" ", "")
                tokenized_list[k] = detail
            standardized_details_list[x].append(tokenized_list)
    return standardized_details_list
def string_to_float_lists(string_lists):
    standardized_float_list = []
    for x in range(0,len(string_lists)): #<- x = 1 entire timeline
        standardized_float_list.append([]) # create timeline entry
        for y in range(0,len(string_lists[x])): # <- y = 1 instance of activity
            duration = float(string_lists[x][y])
            standardized_float_list[x].append(duration)
    return standardized_float_list
def multi_to_single_word_list(word_list):
    standardized_word_list = []
    for x in range(0,len(word_list)): #<- x = 1 entire timeline
        standardized_word_list.append([]) # create timeline entry
        for y in range(0,len(word_list[x])): # <- y = 1 instance of activity
            word = word_list[x][y].replace(" ","")
            word = word.lower()
            standardized_word_list[x].append(word)
    return standardized_word_list
def path_lists_to_token_list(path_list): # in: a list of all paths, out: a list of lists containing the paths seperated by the '\'

    standardized_path_list = []
    for x in range(0,len(path_list)): #<- x = 1 entire timeline
        standardized_path_list.append([]) # create timeline entry
        for y in range(0,len(path_list[x])): # <- y = 1 instance of activity
            standardized_path_list[x].append(split_path_data_to_tokens(path_list[x][y]))
    return standardized_path_list
def split_path_data_to_tokens(path_data):
    # drop first 3 chars (for windows drive)  + split paths as a list of strings by '/' (token)
    if(path_data[:3] == 'C:\\'):
        path_data = path_data[3:]
    path_data = re.split('\\\\|\.',path_data)
    #path_data = np.asarray(path_data)
    for x in range(0, len(path_data)):
        path_data[x] = path_data[x].lower()
    return path_data

## testing
base_dataset = construct_dataset_fromdir(dataset_path)
base_dataset = np.array(base_dataset)
input_lists = standardize_data(base_dataset)

vectorize_feature_datasets(input_lists)

#print(t3.value)
exit()

model = Sequential()
epath_input = keras.layers.Input(shape=(include_lines,),name='epath')

# --- need to do standardization on each feature while keeping the dataset the same shape (REWRITE CURRENT standardization FUNCTIONS TO ACCOMMODATE) use dataset.map


# -- then need to split datasets here post standardization to produce training/test datasets for adapting vectorization layers.


# -- map the training features to appropriate vectorizations


## -- remap the vectored text back to original shape

labels = [
    "BENIGN", # prog 1
    "VIRUS" # prog 2
]

label_e = LabelEncoder()
enc_labels = label_e.fit_transform(labels)

x_train, x_test, y_train, y_test = train_test_split(base_dataset ,enc_labels, test_size=0.5, random_state=55) # <- currently set as .5 as we have 2 programs as test data, one for each dataset.
x_train = np.array(x_train)
print(x_train.shape)

standardized_train = standardize_data(x_train)
standardized_test = standardize_data(x_test)

#vectorize_feature_datasets(standardized_train,standardized_test)


#enc_data = encode_data(data)
#print(enc_data)

'''
outline:
data = dataset_fromdir
x_train, x_test = split(data)

// these following standardized lists are [[ALL FEATURE1 TENSOR],[ALL FEATURE2 TENSOR],..] 
// these must be reshaped into the original datasets shape once vectorization/normalization has taken place.

standard_train = standardize(x_train)  
standard_test = standardize(x_test)

// the vectorization process should take both train and test data simultaniously but only adapt based
// on the training data, this will prevent the model from having any unseen knowledge.

vectorized_train, vectorized_test = vectorize(standard_train,standard_test)

// once vectorization is finished then both datasets should be converted back to the original tensor shape

rebuilt_train, rebuilt_test = rebuild_dataset(vectorized_train, vectorized_test)

// after this there should be 2 datasets 

rebuilt_train=[
    [[feature1_vectorized,feature2_vectorized],[feature1_vectorized,feature2_vectorized]],
    [[feature1_vectorized,feature2_vectorized],[feature1_vectorized,feature2_vectorized]]
]

rebuilt_test=[
    [[feature1_vectorized,feature2_vectorized],[feature1_vectorized,feature2_vectorized]],
    [[feature1_vectorized,feature2_vectorized],[feature1_vectorized,feature2_vectorized]]
]



'''

exit()
enc_data = encode_data(data)
print(enc_data)




## reshape data for LSTM input

x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], -1))
x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], -1))

y_train = np.asarray(y_train).astype('float32').reshape((-1,1))
y_test = np.asarray(y_test).astype('float32').reshape((-1,1))



model = Sequential()

model.add(LSTM(hidden_units,input_shape=(include_lines, 6)))
model.add(Dense(1,activation='sigmoid'))
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
model.summary()
print(model.dtype)
model.fit(x_train, y_train, validation_data=(x_test, y_test),epochs=epochs, batch_size=batch_size)
